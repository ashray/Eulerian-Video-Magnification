<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Eulerian-video-magnification by ashray</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Eulerian-video-magnification</h1>
          <h2>Implement the Eulerian Video Magnification technique</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/ashray/Eulerian-Video-Magnification/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/ashray/Eulerian-Video-Magnification/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/ashray/Eulerian-Video-Magnification" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
          <h3>
<a id="personal-details" class="anchor" href="#personal-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Personal Details</h3>

<p>Name: Ashray Malhotra</p>

<p>University: <a href="http://www.iitb.ac.in">Indian Institute of Technology, Bombay</a></p>

<p>Email: <a href="mailto:ashray.malhotra.1994@gmail.com">ashray.malhotra.1994@gmail.com</a></p>

<p>Telephone: +91-9757163031</p>

<p>Instant Messaging: Google Hangouts(<a href="mailto:ashray.malhotra.1994@gmail.com">ashray.malhotra.1994@gmail.com</a>)</p>

<p>Twitter: <a href="https://twitter.com/ashray_malhotra">https://twitter.com/ashray_malhotra</a></p>

<p>Country of Residence: India</p>

<p>Timezone: IST (GMT + 0530)</p>

<p>Primary Language: English</p>

<p>I am a fourth year dual degree student pursuing B.Tech. + M.Tech. in Electrical Engineering(specialisation in Signal Processing) at Indian Institute of Technology, Bombay. My semester will complete in late April leaving me enough time to get ready for my GSoC project. If I am selected, I shall be able to work around 40 hrs a week on the project, though am open to putting in more effort if the work requires.</p>

<h3>
<a id="why-this-project" class="anchor" href="#why-this-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why this project?</h3>

<p>This technique of motion magnification unlocks completely new avenues like detection of blood vessels,magnify motions of small babies etc. I look at this technology from the perspective of enabling a whole new dimension of use cases. So our aim should be to provide it to users/developers in as flexible form as possible so that people can build upon it for their personal use cases that they can think of.</p>

<p>Another advantage of this technique is that it can work in near real time. Hence it unlocks completely new avenues for technology.</p>

<h3>
<a id="technical-knowledge" class="anchor" href="#technical-knowledge" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technical Knowledge</h3>

<p>I am a 4th year dual degree student in IIT Bombay. I am enrolled in a 5 year B.Tech. + M.Tech. course. My major is Electrical Engineering(it's more of maths and electronics though). My specialisation is in the field of Communications and Signal Processing. The courses that I have done include</p>

<ul>
<li><p>Digital Signal Processing</p></li>
<li><p><a href="http://www.cse.iitb.ac.in/%7Eajitvr/CS663_Fall2014/">Fundamentals of Digital Image Processing</a></p></li>
<li><p>Advanced Computing for Electrical Engineers(A compressed version of important CS courses)</p></li>
<li><p>Advanced Topics in Signal Processing</p></li>
<li><p><a href="http://www.cse.iitb.ac.in/%7Eajitvr/CS763_Spring2015/">Computer Vision</a></p></li>
<li><p><a href="http://www.cse.iitb.ac.in/%7Esuyash/cs736/">Algorithms for Medical Image Processing</a></p></li>
</ul>

<p>I have done many <a href="https://www.coursera.org/course/algo">algorithms</a> and <a href="https://www.coursera.org/course/ml">machine learning</a> courses on coursera.</p>

<p>Some of my previous projects in image processing and computer vision include - </p>

<ul>
<li><p>Dental Imaging Project with MIT Media Lab, built a real time system(designed algorithms) to detect caries(dental cavities) using intra oral camera. This <a href="https://youtu.be/51KwvkdVstE">video</a> summarises the complete work.</p></li>
<li><p>Digit recognition using Adaboost</p></li>
<li><p>Video Stabilisation using RANSAC and least squares on SIFT features</p></li>
<li><p>Denoising MRI images</p></li>
<li><p>Digit Recognition on <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a>. Achieved nearly 91% accuracy with a 100 dimensional subspace(for images of size 28*28 = 784) using PCA technique. Implemented LDA(Fisher's LDA) and ICA techniques.</p></li>
</ul>

<p>Some of my signal processing projects include -</p>

<ul>
<li><p>Source localisation</p></li>
<li><p>Audio Source Seperation</p></li>
<li><p>Speech Recognition System</p></li>
</ul>

<p>Am currently working on many interesting projects, including Iris detection, finding out innovative techniques for improving temporal resolution of a signal(research project with <a href="https://scholar.google.co.in/citations?user=84VkdegAAAAJ&amp;hl=en&amp;oi=ao">Prof Subhasis Chaudhuri</a>) etc. Am also currently working on implementing the video magnification algorithm. Read ahead in the personal motivation section for the reasons why I have already been working on implementing this technique.</p>

<p>I interned with Goldman Sachs technologies in my third year. My work involved extensive use of Java. Goldman involved working with teams across the globe. So I am comfortable working with people across multiple time zones and this shall not be a problem in the development process. I have worked with C++ in my advanced computing course. Based on my skills, I was selected to be a Teaching Assistant for <a href="https://www.edx.org/course/introduction-computer-programming-part-1-iitbombayx-cs101-1x">IIT Bombay's first online course</a>, in which programming was taught in C++.</p>

<p>Programming languages I have previously worked with include C++, Java, Cilk, Python, Matlab, openGL, CUDA, Assembly Language Programming, etc. I have built VTK on my macbook. I also ran a few test codes from the site and it was awesome :D. This was the <a href="https://youtu.be/uNrZHnm4LfU">cylinder test</a>. I have also worked with more advanced tools like <a href="http://www.slicer.org">Slicer</a>.</p>

<h2>
<a id="project" class="anchor" href="#project" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project</h2>

<h3>
<a id="project-abstract" class="anchor" href="#project-abstract" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Abstract</h3>

<p>This project aims to develop algorithms to extract out subtle changes in a time-dependent data set and amplify them. To begin with, the data set can be considered as videos(2D data at each temporal resolution) but scope of the project can be modified to deal with different dimensional datasets at each time instant. We also plan to build custom views for Video magnification in VTK. We extract out temporal and spatial frequencies from the given data and amplify specific frequencies according to our use case.</p>

<h2>
<a id="technical-details" class="anchor" href="#technical-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Technical Details</h2>

<p>Below we have explained the significant steps of the algorithm.</p>

<ul>
<li>We start by considering each of the frame of the video independently for analysis</li>
<li>We choose a suitable colour space in which we want to work, This could depend on the specific application that we are dealing with, though in the paper, authors have used NTSC color space for further operations. </li>
</ul>

<pre><code>frame = rgb2ntsc(rgbframe);
</code></pre>

<ul>
<li>For each of the color level(or spectrum level for hyper spectral images), we build a Laplacian pyramid. Note the Laplacian Pyramid is built of the NTSC image, not the RGB image.</li>
</ul>

<pre><code>[pyr,pind] = buildLaplacianPyramid(frame(:,:,1))
</code></pre>

<ul>
<li>We initialise the lowpass filter to have the Laplacian Pyramid values. Later, we will change the values of the filter limits to perform temporal filtering of the signal.</li>
<li>We consider the next frame, and perform the similar laplacian pyramid calculation on it(after converting in NTSC colour space).</li>
<li>The value of laplacian pyramids of subsequent frames is used to perform the temporal filtering of the signal. The exact method of temporal filtering could vary with application, for ex. we could use a butterworth filter, an IIR filter etc. For an IIR filter, we perform a simple multiplicative update of  both the filter threshold</li>
</ul>

<pre><code>lowpass = (1-const_factor)*lowpass + const_factor*pyramid;
</code></pre>

<ul>
<li>The difference of the computed thresholds gives us the range of frequencies to work with(magnify or suppress, based on the laplacian pyramid level)</li>
</ul>

<pre><code>filtered = (cutoff1 - cutoff2);
</code></pre>

<ul>
<li>Now we have performed the temporal selection of the signal. We will selectively perform the spatial magnification of the signal. Note that the equation that we will use to magnify the spatial frequencies(amount of magnification of a specific spatial frequency), can vary across different use cases, but in this paper, the authors have used linearly increasing magnification with spatial wavelengths with a specific threshold after which the magnification remains constant.
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Magnification%20Curve%20Used%20in%20Paper.png" alt="Magnification Curve">
</li>
<li>The above figure gives us the magnification value(Y axis) at each spatial wavelength(X axis) level(which is given by the pyramid index). We multiply the filtered signal above by the appropriate multiplication(or magnification) factor to get the modified filter values. Note that we will have to do this for all the images in the pyramid.</li>
<li>Using these final filtered values, we again recreate the image frame from the new image pyramid.</li>
<li>We can also consider adding some chromatic aberration if we either want to mix the motion magnified signal and the original signal better(more homogeneously without weird colour artifacts) or we want to show a clear motion in the subsequent frames hence clearly separate the motion magnification and the frame(we can have a contrast between them). Which of the two cases happens will depend on the exact method(and value) of chromatic aberration. </li>
</ul>

<pre><code>output(:,:,1) = output(:,:,1)*chromaticAttenuation1; //Red channel chromatic attenuation
output(:,:,2) = output(:,:,2)*chromaticAttenuation2; //Green channel chromatic attenuation
output(:,:,3) = output(:,:,3)*chromaticAttenuation3; //Blue channel chromatic attenuation
</code></pre>

<ul>
<li>So we finally have the magnified motion. But we need the magnified motion on the image. Hence we add this magnified motion to the original input frame to get the output frame which we will write back to the output video.</li>
</ul>

<pre><code>output = frame + output;
</code></pre>

<h2>
<a id="practically-observing-outputs-of-the-above-algorithm" class="anchor" href="#practically-observing-outputs-of-the-above-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Practically observing outputs of the above algorithm</h2>

<p>To understand the algorithm completely and to verify if the outputs are intuitive, I ran the code and I am summarising the results below.</p>

<p>A baby <a href="https://github.com/ashray/Eulerian-Video-Magnification/blob/master/baby.mp4">sample video</a> was used to generate the images below.</p>

<p>Below is the second frame of the video. This is what goes as an input to our algorithm.
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Original_Frame_2.jpg" alt="Frame 2 of baby video"></p>

<p>The algorithm find out the difference between frame 1 and frame 2 and gives us the following output
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Frame1_2_difference.jpg" alt="Frame 1-2 difference"></p>

<p>Note that the above difference image had to be scaled to normalise it to cover the entire intensity levels otherwise its very difficult to see the difference and we only observe a black image. But one thing is really interesting in this image, that amidst the random squares that we observe, there is one straight red line, nearly in the centre of the image. If we try to match this to out input image, we will observe that this line belongs to the chain of the child's clothes. But as it would be obvious here, if we add this motion to the original frame, the resultant frame will be really bad since we see lots of random areas where there was ideally zero motion.</p>

<p>To solve this problem, we use chromatic aberration factors. We can see that most of the useful information is in red here, so we suppress the other color channels(by multiplying them by 0.1, so reducing them to 10% of original value). After performing the above calculation, the frame becomes
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Frame1_2_difference_chromatic_correction.jpg" alt="Frame 1-2 difference chromatic corrected"></p>

<p>It is very clear now that the major motion is the one of the child's zip. This should be expected because other areas are more continuous, hence have a lesser frequency components, whereas a zip is like a discontinuity which contributes multiple frequencies and hence has a higher contribution to the difference frame.</p>

<p>We add the above frame to our original frame(in the RGB domain) and we get the final frame(for the output video) below.
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Final_Frame_2.jpg" alt="Final Processed 2nd frame"></p>

<p>If we closely notice, some of the bad areas in the image(patches in smooth areas) are exactly the same as what were observed in the final difference image(after chromatic aberrations).</p>

<p>Now, we would expect that the difference image that we get for nearby frames should be similar(since we aren't allowing for infinite frequency changes) and should be more different for frames which are further apart.</p>

<p>This claim is verified by the 3 images below.
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Frame1_2_difference_chromatic_correction.jpg" alt="Frame 1-2 difference">
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Frame2_3_difference_chromatic_correction.jpg" alt="Frame 2-3 difference">
<img src="https://raw.githubusercontent.com/ashray/Eulerian-Video-Magnification/master/Frame29_30_difference_chromatic_correction.jpg" alt="Frame 29-30 difference"></p>

<p>We also see much lesser artefacts in frame 29-30 difference because after some time, the error caused by the initial condition nullifies.</p>

<p>Clearly we can see that the above technique is able to identify the motion in the frame(we can say this because it identifies the child's body motion, or the motion of his zip and neglects the other bodies in the frame). Also as we have seen the new created frames aren't that big a problem to the video quality(artefacts).</p>

<p>The final processed video was created by running the complete code. Output video can be found <a href="https://github.com/ashray/Eulerian-Video-Magnification/blob/master/baby_processed_video.avi">here</a>.</p>

<p>Which basically proves Video Magnification is awesome :D</p>

<h2>
<a id="timeline" class="anchor" href="#timeline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Timeline</h2>

<h3>
<a id="pre-gsoc" class="anchor" href="#pre-gsoc" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pre GSOC</h3>

<p>Implement the video magnification algorithm. I am even currently working on the algorithm as my course project and this will have to be completed before April end, which means before the start of GSOC project. This also easies my task in those 12 weeks and helps me concentrate on shipping professional code,</p>

<h3>
<a id="community-bonding" class="anchor" href="#community-bonding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Community Bonding</h3>

<p>Getting acquainted with the code base of VTK and the procedure that needs to be followed to submit code and get it reviewed. Discussing with the team on what exactly needs to be the problem statement(minute details, like kind of dataset we want to use, what parameters should be user choices, like magnification factors, what should be algorithmically decided based on input data type, etc).</p>

<h3>
<a id="week-1" class="anchor" href="#week-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 1</h3>

<p>Understand the relevant parts of the VTK code base and try to figure out how the final product should look like, how many and what kind of views should be made etc.</p>

<h3>
<a id="week-2" class="anchor" href="#week-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 2</h3>

<p>Begin implementing magnifying motions. In this week, focus on implementing spatial pyramids, given an image using Gaussian and Laplacian pyramid techniques. Work load has been kept less this week so that I can practice other crucial techniques such as adding test cases and understanding code review work flow.</p>

<h3>
<a id="week-3" class="anchor" href="#week-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 3</h3>

<p>Finally get to video(and/or some other temporally varying data if the team decides to tackle that use case). Understand temporal filtering of signals and begin coding temporal filtering of signals.</p>

<h3>
<a id="week-5-6" class="anchor" href="#week-5-6" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 5-6</h3>

<p>Code all different kind of temporal filters(IIR, Ideal etc, Which ones to implement will be decided in the community bonding period. My personal choice would be to code the 4 used in the paper). Also this would be the mid term review period.</p>

<h3>
<a id="week-7-8" class="anchor" href="#week-7-8" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 7-8</h3>

<p>Once we have both the spatial and temporal filters ready, to combine these two codes is easy. We would need to spend time though trying to optimise parameters for specific use cases. We can perform enough number of experiments to come up with a guideline or a rule book on how to choose the parameters. This would be useful to developers when they want to use this code for their applications and use cases.</p>

<h3>
<a id="week-9-10" class="anchor" href="#week-9-10" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 9-10</h3>

<p>Start working on the designing views for implementation of this code in the toolkit.</p>

<h3>
<a id="week-11-12" class="anchor" href="#week-11-12" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 11-12</h3>

<p>Take feedback from the community and iterate on the designs and improvise on use cases. Ensure code quality by adding more test cases and working with more videos. Work to make document, blogs or videos to help increase the user base for this product(Subject to developer community approval).</p>

<h3>
<a id="week-13" class="anchor" href="#week-13" aria-hidden="true"><span class="octicon octicon-link"></span></a>Week 13</h3>

<p>Spare week in case of some work getting delayed, in case of any emergency or otherwise.</p>

<h3>
<a id="personal-inspiration-for-the-project" class="anchor" href="#personal-inspiration-for-the-project" aria-hidden="true"><span class="octicon octicon-link"></span></a>Personal Inspiration for the Project</h3>

<p>I am really excited to work on the idea of video magnification. I have been following that topic since really long, first came to know about it from their <a href="http://www.ted.com/talks/michael_rubinstein_see_invisible_motion_hear_silent_sounds_cool_creepy_we_can_t_decide?language=en">TED talk</a>, which inspired me to read their <a href="http://people.csail.mit.edu/mrub/vidmag/#publications">research paper</a>. I was also working in the week long <a href="https://mitredxcampjan2015.wordpress.com">ReDX camp</a> conducted by MIT Media lab(conducted by Prof Ramesh Raskar). One of the projects in that lab was to find out the human arteries. I was shocked to find out that doctors still need to stop the blood flow and then find the arteries. According to some data that was collected during the workshop, it could take upto 7 minutes to find an infants(or older people’s) correct blood vessels; in a crucial operation, this could be lethal. The team in that week had come up with an IR device which would assist the RGB camera of the cellphone and using Augmented Reality would superimpose the blood vessels(visible in IR) on to the RGB smartphone camera.</p>

<p>But I still had a problem with this solution, that we would still need a separate hardware, it could not be user friendly etc. So I wanted to apply the concept of Video Magnification to be able to magnify the blood vessel movement. This way, the injections could be correct in the first attempt and in no time.</p>

<p>So all of this was on back of my mind, churning, but my final push came from my personal experience. My grandfather had died a year ago, and when he had been taken to the hospital, the needles and stuff would pain him so much that he died in a lot of pain. From the horrible experience we had with him, my grandmother, who expired just a few months ago, was not even taken to the hospital and she died very calmly in the house. To me, this was a shock, because even in this era if people believe that hospitals actually make people’s life worse, then whats the entire point of health technology.</p>

<p>We all know that video magnification can solve this simple problem, the only issue is that it seems no one has actually put in effort to get it to the people. When I came back from my home town after this incident, I decided to take this challenge on myself. I decided to implement this technology myself, out of interest and as a course project in my Medical Image Processing course. By April end(this year), I would have myself implemented this technique(which would mean I would completely know how this works, not just have a theoretical understanding). I will be free in summers and even in the final year(5th year), we just have a research project and a course or two. It is my aim to be able to get this technology in the hands of as many people as I possibly can. I already had plans to work on an implementation of this technology in these summer vacations. I would be glad if I could do this as a part of GSOC along with Kitware.</p>

<p>I have a huge personal inspiration to get this technology out to the world, and you can be assured of my motivation to complete this project.</p>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h3>

<p>Most of the content including images and input video for this document have been taken from the <a href="http://people.csail.mit.edu/mrub/vidmag/">website</a> of the authors of the paper.</p>
        </section>

        <footer>
          Eulerian-video-magnification is maintained by <a href="https://github.com/ashray">ashray</a><br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

                  <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-61169459-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

      </div>
    </div>
  </body>
</html>